{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de108df0",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc988568",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/Users/avatarjoshi/miniforge3/envs/tfenv/bin/python\"\n  * The NumPy version is: \"1.22.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): Library not loaded: '@rpath/libcblas.3.dylib'\n  Referenced from: '/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so'\n  Reason: tried: '/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/avatarjoshi/miniforge3/envs/tfenv/bin/../lib/libcblas.3.dylib' (no such file), '/Users/avatarjoshi/miniforge3/envs/tfenv/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import dependencies\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "File \u001b[0;32m~/miniforge3/envs/tfenv/lib/python3.10/site-packages/pandas/__init__.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _missing_dependencies:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to import required dependencies:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_missing_dependencies)\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to import required dependencies:\nnumpy: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.10 from \"/Users/avatarjoshi/miniforge3/envs/tfenv/bin/python\"\n  * The NumPy version is: \"1.22.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so, 0x0002): Library not loaded: '@rpath/libcblas.3.dylib'\n  Referenced from: '/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/_multiarray_umath.cpython-310-darwin.so'\n  Reason: tried: '/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/avatarjoshi/miniforge3/envs/tfenv/lib/python3.10/site-packages/numpy/core/../../../../libcblas.3.dylib' (no such file), '/Users/avatarjoshi/miniforge3/envs/tfenv/bin/../lib/libcblas.3.dylib' (no such file), '/Users/avatarjoshi/miniforge3/envs/tfenv/bin/../lib/libcblas.3.dylib' (no such file), '/usr/local/lib/libcblas.3.dylib' (no such file), '/usr/lib/libcblas.3.dylib' (no such file)\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "# import seaborn as sb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "# from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PLACES data\n",
    "places_df = pd.read_csv(\"./Resources/processed_PLACES_COPD.csv\")\n",
    "places_df = places_df.drop([\"State_County\"], axis=1)\n",
    "places_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Coal Mines data\n",
    "coal_df = pd.read_csv(\"./Resources/processed_Coal_Mines.csv\")\n",
    "coal_df = coal_df.drop([\"State_County\"], axis=1)\n",
    "coal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Air Quality data\n",
    "aqi_df = pd.read_csv(\"./Resources/processed_Decade_Air_Quality.csv\")\n",
    "aqi_df = aqi_df.drop([\"State_County\"], axis=1)\n",
    "aqi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Census 2019 data\n",
    "census_df = pd.read_csv(\"./Resources/processed_census_data.csv\")\n",
    "census_df = census_df.drop([\"State_County\"], axis=1)\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge places and coal dataframes\n",
    "merged_df = pd.merge(places_df, coal_df, on=[\"State\", \"County\"], how=\"left\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70272d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the Air Quality data\n",
    "merged_df = pd.merge(merged_df, aqi_df, on=[\"State\", \"County\"], how=\"left\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the census data\n",
    "merged_df = pd.merge(merged_df, census_df, on=[\"State\", \"County\"], how=\"left\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View null values\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23888a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the county with a NaN value\n",
    "merged_df[merged_df['County'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row containing \"United States\"\n",
    "merged_df = merged_df[merged_df.State != \"United States\"]\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Nulls from states without mines to be 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop County name as it might confuse the model\n",
    "merged_df = merged_df.drop([\"County\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72ce0e",
   "metadata": {},
   "source": [
    "# Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e438dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for label encoding\n",
    "le = LabelEncoder()\n",
    "encoded_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ae7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all string object data types into a dataframe_cat for encoding\n",
    "dataframe_cat = merged_df.dtypes[merged_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "dataframe_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23173a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to encode text columns to numerical values\n",
    "for column in dataframe_cat:\n",
    "    encoded_df[column] = le.fit_transform(merged_df[column])\n",
    "\n",
    "\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa5c7d",
   "metadata": {},
   "source": [
    "# Split, Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bbeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "X = encoded_df.drop(columns = \"Levels_COPD\").values\n",
    "y = encoded_df[\"Levels_COPD\"]\n",
    "\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b89cd",
   "metadata": {},
   "source": [
    "# Find Correlation Between Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10052daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = encoded_df[[\"State\", \"Levels_Smokers\", \"Surface_Mines\", \n",
    "                             \"Underground_Mines\", \"Good_Days\", \"Levels_COPD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3039b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "heatmap = sb.heatmap(correlation_df.corr(), vmin=-1, vmax=1, annot=True, cmap='RdBu')\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);\n",
    "# save heatmap as .png file\n",
    "# dpi - sets the resolution of the saved image in dots/inches\n",
    "# bbox_inches - when set to 'tight' - does not allow the labels to be cropped\n",
    "plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = len(X_train[0]) / 2\n",
    "hidden_nodes_layer2 = len(X_train[0]) / 3\n",
    "hidden_nodes_layer3 = len(X_train[0]) / 4\n",
    "# hidden_nodes_layer4 = len(X_train[0]) / 5\n",
    "# hidden_nodes_layer5 = len(X_train[0]) / 6\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# # Fourth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
    "\n",
    "# # Fifth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import checkpoint dependencies\n",
    "# import os\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Define the checkpoint path and filenames\n",
    "# os.makedirs(\"optimized_checkpoints/\",exist_ok=True)\n",
    "# checkpoint_path = \"optimized_checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6314cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# # Create a callback that saves the model's weights every 5 epochs\n",
    "# cp_callback = ModelCheckpoint(\n",
    "#     filepath = checkpoint_path,\n",
    "#     verbose = 1,\n",
    "#     save_weights_only = True,\n",
    "#     save_freq = 5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e308971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# fit_model = nn.fit(X_train_scaled, y_train, epochs=1, callbacks=[cp_callback])\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model using the test data\n",
    "# model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "# print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83018a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export our model to HDF5 file\n",
    "# nn.save('COPD_DeepLearning.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179798c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
