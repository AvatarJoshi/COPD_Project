{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de108df0",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc988568",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import dependencies\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tfenv/lib/python3.10/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/miniforge3/envs/tfenv/lib/python3.10/site-packages/pandas/compat/__init__.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tfenv/lib/python3.10/site-packages/pandas/_typing.py:79\u001b[0m\n\u001b[1;32m     75\u001b[0m HashableT \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHashableT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mHashable)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# array-like\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m ArrayLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtensionArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m]\n\u001b[1;32m     80\u001b[0m AnyArrayLike \u001b[38;5;241m=\u001b[39m Union[ArrayLike, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# scalars\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ndarray'"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "# from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PLACES data\n",
    "places_df = pd.read_csv(\"./Resources/processed_PLACES_COPD.csv\")\n",
    "places_df = places_df.drop([\"State_County\"], axis=1)\n",
    "places_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Coal Mines data\n",
    "coal_df = pd.read_csv(\"./Resources/processed_Coal_Mines.csv\")\n",
    "coal_df = coal_df.drop([\"State_County\"], axis=1)\n",
    "coal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Air Quality data\n",
    "aqi_df = pd.read_csv(\"./Resources/processed_Decade_Air_Quality.csv\")\n",
    "aqi_df = aqi_df.drop([\"State_County\"], axis=1)\n",
    "aqi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Census 2019 data\n",
    "census_df = pd.read_csv(\"./Resources/processed_census_data.csv\")\n",
    "census_df = census_df.drop([\"State_County\"], axis=1)\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge places and coal dataframes\n",
    "merged_df = pd.merge(places_df, coal_df, on=[\"State\", \"County\"], how=\"left\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70272d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the Air Quality data\n",
    "merged_df = pd.merge(merged_df, aqi_df, on=[\"State\", \"County\"], how=\"left\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the census data\n",
    "merged_df = pd.merge(merged_df, census_df, on=[\"State\", \"County\"], how=\"left\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View null values\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23888a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the county with a NaN value\n",
    "merged_df[merged_df['County'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row containing \"United States\"\n",
    "merged_df = merged_df[merged_df.State != \"United States\"]\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Nulls from states without mines to be 0\n",
    "merged_df = merged_df.fillna(0)\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop County name as it might confuse the model\n",
    "merged_df = merged_df.drop([\"County\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72ce0e",
   "metadata": {},
   "source": [
    "# Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e438dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for label encoding\n",
    "le = LabelEncoder()\n",
    "encoded_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65ae7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all string object data types into a dataframe_cat for encoding\n",
    "dataframe_cat = merged_df.dtypes[merged_df.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "dataframe_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23173a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop to encode text columns to numerical values\n",
    "for column in dataframe_cat:\n",
    "    encoded_df[column] = le.fit_transform(merged_df[column])\n",
    "\n",
    "\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa5c7d",
   "metadata": {},
   "source": [
    "# Split, Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bbeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "X = encoded_df.drop(\"Levels_COPD\", axis=1)\n",
    "y = encoded_df[\"Levels_COPD\"]\n",
    "\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b89cd",
   "metadata": {},
   "source": [
    "# Find Correlation Between Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10052daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_df = encoded_df[[\"State\", \"Levels_Smokers\", \"Surface_Mines\", \n",
    "                             \"Underground_Mines\", \"Good_Days\", \"Levels_COPD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3039b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "heatmap = sb.heatmap(correlation_df.corr(), vmin=-1, vmax=1, annot=True, cmap='RdBu')\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=12);\n",
    "# save heatmap as .png file\n",
    "# dpi - sets the resolution of the saved image in dots/inches\n",
    "# bbox_inches - when set to 'tight' - does not allow the labels to be cropped\n",
    "plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = len(X_train[0]) / 2\n",
    "hidden_nodes_layer2 = len(X_train[0]) / 3\n",
    "hidden_nodes_layer3 = len(X_train[0]) / 4\n",
    "# hidden_nodes_layer4 = len(X_train[0]) / 5\n",
    "# hidden_nodes_layer5 = len(X_train[0]) / 6\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(\n",
    "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
    ")\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# # Fourth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
    "\n",
    "# # Fifth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb61e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import checkpoint dependencies\n",
    "# import os\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Define the checkpoint path and filenames\n",
    "# os.makedirs(\"optimized_checkpoints/\",exist_ok=True)\n",
    "# checkpoint_path = \"optimized_checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6314cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# # Create a callback that saves the model's weights every 5 epochs\n",
    "# cp_callback = ModelCheckpoint(\n",
    "#     filepath = checkpoint_path,\n",
    "#     verbose = 1,\n",
    "#     save_weights_only = True,\n",
    "#     save_freq = 5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e308971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# fit_model = nn.fit(X_train_scaled, y_train, epochs=1, callbacks=[cp_callback])\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model using the test data\n",
    "# model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "# print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83018a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export our model to HDF5 file\n",
    "# nn.save('COPD_DeepLearning.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad06257",
   "metadata": {},
   "source": [
    "# Determine Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4584384e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns, \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m), key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m cols \u001b[38;5;241m=\u001b[39m [f[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m      3\u001b[0m width \u001b[38;5;241m=\u001b[39m [f[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "features = sorted(zip(X.columns, nn.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10,200)\n",
    "plt.margins(y=0.001)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179798c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
